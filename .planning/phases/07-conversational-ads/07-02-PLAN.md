---
phase: 07-conversational-ads
plan: 02
type: execute
depends_on: ["07-01"]
files_modified: [src/lib/chat.ts, src/app/api/chat/route.ts, src/components/chat-widget.tsx]
---

<objective>
Create Gemini chat backend with streaming responses.

Purpose: Enable real-time conversational AI responses in the chat widget using Gemini.
Output: Chat API endpoint with streaming, chat library for context management, widget integration.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-conversational-ads/07-CONTEXT.md
@.planning/phases/07-conversational-ads/07-01-SUMMARY.md

**From CONTEXT.md - Vision:**
- Smart recommendations based on conversation
- Feels like talking to a knowledgeable sales assistant
- Brand-aware: uses brand tone/voice from ingested guidelines

**Existing Gemini integration:**
- src/lib/gemini.ts: Uses @google/generative-ai SDK
- Model: gemini-1.5-flash (fast, cost-effective)
- Pattern: generateContent() with JSON parsing
- Can upgrade to generateContentStream() for streaming

**API patterns:**
- Standard Next.js API routes with NextResponse
- No streaming currently - this plan adds it
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat library with context management</name>
  <files>src/lib/chat.ts</files>
  <action>
Create src/lib/chat.ts:

```typescript
import { GoogleGenerativeAI } from '@google/generative-ai';

// Initialize Gemini for chat
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY!);

// Chat message type
export interface ChatMessage {
  role: 'user' | 'assistant';
  content: string;
}

// Context for brand-aware responses
export interface ChatContext {
  brandName?: string;
  brandTone?: string[];     // Tone descriptors from brand
  productNames?: string[];  // Available products for recommendations
}

// Build system prompt with context
export function buildChatSystemPrompt(context: ChatContext): string {
  let prompt = `You are a helpful product discovery assistant for an online store.
Your goal is to understand what visitors are looking for and help them find the right products.

Guidelines:
- Be conversational and friendly
- Ask clarifying questions to understand needs
- Make specific product recommendations when you have enough information
- Keep responses concise (2-3 sentences max unless explaining products)
- Never make up product information - only recommend products from the catalog`;

  if (context.brandName) {
    prompt += `\n\nYou represent ${context.brandName}.`;
  }

  if (context.brandTone?.length) {
    prompt += `\n\nBrand voice: ${context.brandTone.join(', ')}.`;
  }

  if (context.productNames?.length) {
    prompt += `\n\nAvailable products: ${context.productNames.join(', ')}.`;
  }

  return prompt;
}

// Create streaming chat response
export async function* streamChatResponse(
  messages: ChatMessage[],
  context: ChatContext
): AsyncGenerator<string, void, unknown> {
  const model = genAI.getGenerativeModel({ model: 'gemini-1.5-flash' });

  const systemPrompt = buildChatSystemPrompt(context);

  // Build chat history for Gemini
  const history = messages.slice(0, -1).map(msg => ({
    role: msg.role === 'user' ? 'user' : 'model',
    parts: [{ text: msg.content }],
  }));

  const chat = model.startChat({
    history,
    systemInstruction: systemPrompt,
  });

  // Get the last user message
  const lastMessage = messages[messages.length - 1];

  // Stream the response
  const result = await chat.sendMessageStream(lastMessage.content);

  for await (const chunk of result.stream) {
    const text = chunk.text();
    if (text) {
      yield text;
    }
  }
}
```

**Key points:**
- Uses Gemini's chat API with history for multi-turn conversations
- System instruction sets the assistant personality and constraints
- Context injection for brand tone and product catalog
- AsyncGenerator for streaming chunks
  </action>
  <verify>npm run build succeeds, types compile correctly</verify>
  <done>Chat library with streaming and context management ready</done>
</task>

<task type="auto">
  <name>Task 2: Create streaming chat API endpoint</name>
  <files>src/app/api/chat/route.ts</files>
  <action>
Create src/app/api/chat/route.ts:

```typescript
import { NextRequest } from 'next/server';
import { streamChatResponse, ChatMessage, ChatContext } from '@/lib/chat';
import { createClient } from '@/lib/supabase/server';

export async function POST(request: NextRequest) {
  try {
    const { messages, brandId } = await request.json() as {
      messages: ChatMessage[];
      brandId?: string;
    };

    if (!messages?.length) {
      return new Response('Messages required', { status: 400 });
    }

    // Build context from brand if provided
    let context: ChatContext = {};

    if (brandId) {
      const supabase = await createClient();

      // Fetch brand with tone
      const { data: brand } = await supabase
        .from('brands')
        .select('name, tone:brand_tone(descriptor)')
        .eq('id', brandId)
        .single();

      if (brand) {
        context.brandName = brand.name;
        context.brandTone = brand.tone?.map((t: { descriptor: string }) => t.descriptor) || [];
      }

      // Fetch products for this brand
      const { data: products } = await supabase
        .from('products')
        .select('name')
        .eq('brand_id', brandId);

      if (products?.length) {
        context.productNames = products.map(p => p.name);
      }
    }

    // Create streaming response
    const encoder = new TextEncoder();

    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const chunk of streamChatResponse(messages, context)) {
            controller.enqueue(encoder.encode(chunk));
          }
          controller.close();
        } catch (error) {
          controller.error(error);
        }
      },
    });

    return new Response(stream, {
      headers: {
        'Content-Type': 'text/plain; charset=utf-8',
        'Transfer-Encoding': 'chunked',
      },
    });
  } catch (error) {
    console.error('Chat API error:', error);
    return new Response('Internal server error', { status: 500 });
  }
}
```

**Key points:**
- Accepts messages array and optional brandId
- Fetches brand context (name, tone, products) from Supabase
- Returns ReadableStream for real-time response
- Proper error handling with try/catch
  </action>
  <verify>npm run build succeeds, endpoint accessible</verify>
  <done>Streaming chat API endpoint working</done>
</task>

<task type="auto">
  <name>Task 3: Integrate streaming with chat widget</name>
  <files>src/components/chat-widget.tsx</files>
  <action>
Update src/components/chat-widget.tsx to connect to the API:

**Add streaming fetch logic:**

```typescript
const sendMessage = async () => {
  if (!inputValue.trim() || isLoading) return;

  const userMessage: Message = {
    id: crypto.randomUUID(),
    role: 'user',
    content: inputValue.trim(),
    timestamp: new Date(),
  };

  setMessages(prev => [...prev, userMessage]);
  setInputValue('');
  setIsLoading(true);

  // Create placeholder for assistant response
  const assistantId = crypto.randomUUID();
  setMessages(prev => [...prev, {
    id: assistantId,
    role: 'assistant',
    content: '',
    timestamp: new Date(),
  }]);

  try {
    const response = await fetch('/api/chat', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        messages: [...messages, userMessage].map(m => ({
          role: m.role,
          content: m.content,
        })),
        brandId,
      }),
    });

    if (!response.ok) throw new Error('Chat request failed');

    const reader = response.body?.getReader();
    const decoder = new TextDecoder();

    if (reader) {
      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        const chunk = decoder.decode(value);

        // Update the assistant message with new content
        setMessages(prev => prev.map(m =>
          m.id === assistantId
            ? { ...m, content: m.content + chunk }
            : m
        ));
      }
    }
  } catch (error) {
    console.error('Chat error:', error);
    // Update assistant message with error
    setMessages(prev => prev.map(m =>
      m.id === assistantId
        ? { ...m, content: 'Sorry, I encountered an error. Please try again.' }
        : m
    ));
  } finally {
    setIsLoading(false);
  }
};
```

**Also update:**
- Auto-scroll to bottom when new messages arrive (useEffect with ref)
- Loading indicator while streaming (show typing dots until first chunk)
- Pass brandId prop through to API call
  </action>
  <verify>npm run build succeeds, chat widget streams responses in real-time</verify>
  <done>Chat widget connected to Gemini with streaming responses</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] `npm run build` succeeds
- [ ] Chat library exports correctly
- [ ] /api/chat endpoint responds with streaming
- [ ] Widget sends messages and receives streaming responses
- [ ] Brand context (tone) affects response style
- [ ] Multi-turn conversation works (history preserved)
- [ ] Error states handled gracefully
- [ ] No TypeScript or ESLint errors
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- Chat widget connects to Gemini backend
- Responses stream in real-time (not batch)
- Brand tone influences assistant personality
- Ready for product recommendation logic in Plan 03
</success_criteria>

<output>
After completion, create `.planning/phases/07-conversational-ads/07-02-SUMMARY.md`

Commit guidance:
- Task 1: `feat(07-02): create chat library with streaming`
- Task 2: `feat(07-02): create streaming chat API endpoint`
- Task 3: `feat(07-02): integrate streaming with chat widget`
- Metadata: `docs(07-02): complete Gemini chat backend plan`
</output>
